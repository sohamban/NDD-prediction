{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8960545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169df05a",
   "metadata": {},
   "source": [
    "# Grouped ICA based Time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26657067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ICA based time series data and label\n",
    "time_series1=np.load(\"E:/ADHD200/ICA_times_series_905.npy\",allow_pickle=True)\n",
    "label= np.load(\"E:/ADHD200/label_905.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b36112",
   "metadata": {},
   "source": [
    "# Grouped DL based time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e99be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load DL based time series data and label\n",
    "time_series2=np.load(\"E:/ADHD200/dict_learning_times_series_905.npy\",allow_pickle=True)\n",
    "label= np.load(\"E:/ADHD200/label_905.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd2c096",
   "metadata": {},
   "source": [
    "# Concatenate time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b79e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = []\n",
    "for i in range(len(time_series1)):\n",
    "    #concatenate the time series data of grouped ICA and DL\n",
    "   time_series.append(np.concatenate((time_series1[i], time_series2[i]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffea464f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 93)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(time_series[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1658e82e",
   "metadata": {},
   "source": [
    "# Time domain feature set generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f57eee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "def entropy_cal(data):\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Calculate the probability distribution\n",
    "    unique_elements, counts = np.unique(data, return_counts=True)\n",
    "    probabilities = counts / len(data)\n",
    "    \n",
    "    # Calculate the entropy\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy_value\n",
    "\n",
    "def calculate_skewness(data):\n",
    "    # Convert data to a numpy array if it's not already\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Calculate skewness\n",
    "    skewness_value = skew(data)\n",
    "    \n",
    "    return skewness_value\n",
    "\n",
    "def calculate_kurtosis(data):\n",
    "    # Convert data to a numpy array if it's not already\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Calculate mean, standard deviation, and kurtosis\n",
    "    mean_val = np.mean(data)\n",
    "    std_val = np.std(data, ddof=1)  # Use ddof=1 for sample standard deviation\n",
    "    kurtosis_value = np.mean((data - mean_val) ** 4) / std_val ** 4\n",
    "    \n",
    "    return kurtosis_value\n",
    "\n",
    "a=[]\n",
    "one_roi=[]\n",
    "all_sub=[]\n",
    "for i in range(0,len(time_series)):\n",
    "        for j in range(0,time_series[i].shape[1]):\n",
    "            a.append(np.mean(time_series[i][:,j]))\n",
    "            a.append(np.std(time_series[i][:,j]))\n",
    "            a.append(entropy_cal(time_series[i][:,j]))\n",
    "            a.append(calculate_skewness(time_series[i][:,j]))\n",
    "            a.append(calculate_kurtosis(time_series[i][:,j]))\n",
    "            one_roi.append(a)\n",
    "            a=[]\n",
    "        all_sub.append(one_roi)\n",
    "        one_roi=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "eb598b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941, 93, 5)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(all_sub).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c66d2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save time domain features at drive\n",
    "np.save(\"E:/ADHD200/plv_corr/time_domain_features.npy\", all_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0765789",
   "metadata": {},
   "source": [
    "# Frequency domain feature set generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b64b1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_power_spectral_density(data):\n",
    "    # Convert data to a numpy array if it's not already\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Compute the Power Spectral Density (PSD)\n",
    "    freq_spectrum = np.fft.fft(data)\n",
    "    psd = np.abs(freq_spectrum) ** 2\n",
    "    \n",
    "    # Calculate corresponding normalized frequency values\n",
    "    num_samples = len(data)\n",
    "    frequencies = np.fft.fftfreq(num_samples, d=1/sampling_rate)\n",
    "    \n",
    "    return frequencies, psd\n",
    "\n",
    "def calculate_spectral_entropy(data, fs):\n",
    "    # Convert data to a numpy array if it's not already\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Calculate the power spectral density (PSD)\n",
    "    frequencies, psd = welch(data, fs=fs)\n",
    "    \n",
    "    # Normalize the PSD to obtain a probability distribution\n",
    "    psd_normalized = psd / np.sum(psd)\n",
    "    \n",
    "    # Calculate the spectral entropy\n",
    "    spectral_entropy = entropy(psd_normalized, base=2)\n",
    "    \n",
    "    return spectral_entropy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_frequency_centroid(time_series_data, fs):\n",
    "    # Convert data to a numpy array if it's not already\n",
    "    data = np.array(time_series_data)\n",
    "\n",
    "    # Compute the power spectral density (PSD) using FFT\n",
    "    fft_vals = np.fft.fft(data)\n",
    "    psd = np.abs(fft_vals) ** 2\n",
    "\n",
    "    # Calculate the frequencies corresponding to FFT components\n",
    "    n = len(data)\n",
    "    frequencies = np.fft.fftfreq(n, d=1.0 / fs)\n",
    "\n",
    "    # Calculate the total power\n",
    "    total_power = np.trapz(psd, frequencies)\n",
    "\n",
    "    # Calculate the frequency centroid\n",
    "    frequency_centroid = np.sum(psd * frequencies) / total_power\n",
    "\n",
    "    return frequency_centroid\n",
    "\n",
    "a=[]\n",
    "one_roi=[]\n",
    "all_sub=[]\n",
    "sampling_rate = 2  # Sampling rate (Hz)\n",
    "for i in range(0,len(time_series)):\n",
    "        for j in range(0,time_series[i].shape[1]):\n",
    "            frequencies, psd = calculate_power_spectral_density(time_series[i][:,j])\n",
    "            a.append(psd)\n",
    "            spectral_entropy_result = calculate_spectral_entropy(time_series[i][:,j], sampling_rate)\n",
    "            a.append(spectral_entropy_result)\n",
    "            ctd=calculate_frequency_centroid(time_series[i][:,j],sampling_rate)\n",
    "            a.append(ctd)\n",
    "            one_roi.append(a)\n",
    "            a=[]\n",
    "        all_sub.append(one_roi)\n",
    "        one_roi=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save frequecy domain features at drive\n",
    "np.save(\"E:/ADHD200/plv_corr/frequency_domain_features.npy\", all_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9969ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python_speech_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354555f",
   "metadata": {},
   "source": [
    "# Time-Frequency domain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a37206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pywt\n",
    "#import librosa\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "# Calculate all feature values \n",
    "def time_frequency_entropy(spectrogram_data):\n",
    "    return -np.sum(spectrogram_data * np.log2(spectrogram_data), axis=0)\n",
    "\n",
    "def continuous_wavelet_transform(data):\n",
    "    return pywt.cwt(data, np.arange(1, 128), 'mexh')\n",
    "\n",
    "def time_frequency_variance(cwt_matrix):\n",
    "    return np.var(np.abs(cwt_matrix), axis=0)\n",
    "\n",
    "def wavelet_packet_transform(data):\n",
    "    wp = pywt.WaveletPacket(data, wavelet='db1', mode='symmetric', maxlevel=4)\n",
    "    return np.array([node.data.sum() for node in wp.get_level(4, 'freq')])\n",
    "\n",
    "def mel_frequency_cepstral_coefficients(data, fs):\n",
    "    return librosa.feature.mfcc(data, sr=fs, n_mfcc=13)\n",
    "\n",
    "a=[]\n",
    "one_roi=[]\n",
    "all_sub=[]\n",
    "sampling_rate = 2 \n",
    "for i in range(0,len(time_series)):\n",
    "        for j in range(0,time_series[i].shape[1]):\n",
    "            # Calculate the STFT to obtain the spectrogram\n",
    "             frequencies, times, spectrogram_data = spectrogram(time_series[i][:,j], fs=sampling_rate)\n",
    "            # Calculate individual time-frequency domain features\n",
    "             time_freq_entropy_result = time_frequency_entropy(spectrogram_data)\n",
    "             cwt_matrix, frequencies_cwt = continuous_wavelet_transform(time_series[i][:,j])\n",
    "             time_freq_variance_result = time_frequency_variance(cwt_matrix)\n",
    "             wpt_energy_result = wavelet_packet_transform(time_series[i][:,j])\n",
    "             #mfccs_result = mel_frequency_cepstral_coefficients(time_series[i][:,j], fs=sampling_rate)\n",
    "             a.append(time_freq_entropy_result)\n",
    "             a.append(frequencies_cwt[np.argmax(np.abs(cwt_matrix), axis=0)])\n",
    "             a.append(time_freq_variance_result)\n",
    "             a.append(wpt_energy_result)\n",
    "             #a.append(mfccs_result)\n",
    "             one_roi.append(a)\n",
    "             a=[]\n",
    "        all_sub.append(one_roi)\n",
    "        one_roi=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"E:/ADHD200/plv_corr/time_frequency_domain_features.npy\", all_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60315dff",
   "metadata": {},
   "source": [
    "# PLV based connectivity matrix  generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01140909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# defining phase function \n",
    "def compute_phase(signal):\n",
    "    \"\"\"Compute the phase of the signal using the Hilbert transform.\"\"\"\n",
    "    hilbert_transform = np.fft.fft(signal)\n",
    "    \"\"\"\"gate phas angle.\"\"\"\n",
    "    phase = np.angle(hilbert_transform)\n",
    "    return phase\n",
    "\n",
    "# calculating plv value\n",
    "def compute_plv(signal1, signal2):\n",
    "    \"\"\"Compute Phase-Locking Value (PLV) between two signals.\"\"\"\n",
    "    phase1 = compute_phase(signal1)\n",
    "    phase2 = compute_phase(signal2)\n",
    "\n",
    "    # Compute the phase difference\n",
    "    phase_difference = np.angle(np.exp(1j * (phase1 - phase2)))\n",
    "\n",
    "    # Compute the PLV\n",
    "    plv = np.abs(np.mean(np.exp(1j * phase_difference)))\n",
    "\n",
    "    return plv\n",
    "\n",
    "# plv based matrix generation\n",
    "# time_series stores the concatenated time series data \n",
    "each_roi=[]\n",
    "all_roi=[]\n",
    "all_sub=[]\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(0,len(time_series)):\n",
    "        for j in range(0,time_series[i].shape[1]):\n",
    "            for k in range(0, time_series[i].shape[1]):\n",
    "                    plv_value = compute_plv(time_series[i][:,j], time_series[i][:,k])\n",
    "                    each_roi.append(plv_value)\n",
    "            all_roi.append(each_roi)       \n",
    "            each_roi=[]\n",
    "        all_sub.append(all_roi)\n",
    "        all_roi=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e46434b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data in drive \n",
    "np.save(\"E:/ADHD200/plv_corr/all_plv.npy\", all_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38479533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X= np.load(\"E:/ADHD200/plv_corr/all_plv.npy\")\n",
    "Y= np.load(\"E:/ADHD200/label_941.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af770f54",
   "metadata": {},
   "source": [
    "# Threshold calculation and adjacency matrix generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd0d5574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# load data\n",
    "X= np.load(\"E:/ADHD200/plv_corr/all_plv.npy\")\n",
    "Y= np.load(\"E:/ADHD200/label_905.npy\")\n",
    "\n",
    "X_mean = np.mean(abs(X), axis=0)\n",
    "sum_of_values = np.sum(X_mean)\n",
    "\n",
    "#calculate threshold by averaging the mean value of connectivity matrix\n",
    "threshold= sum_of_values/(X_mean.shape[0]*X_mean.shape[1])\n",
    "threshold\n",
    "\n",
    "\n",
    "x_adhd=[]\n",
    "x_tc=[]\n",
    "for i in range(0,len(X)):\n",
    "    if(Y[i]==1):\n",
    "        x_adhd.append(X[i])\n",
    "    else:\n",
    "        x_tc.append(X[i])\n",
    "\n",
    "\n",
    "x_mean_adhd = np.mean(x_adhd, axis=0)\n",
    "x_mean_tc = np.mean(x_tc, axis=0)\n",
    "\n",
    "weighted_adj_adhd_mean = np.where(x_mean_adhd < threshold, 0, x_mean_adhd)\n",
    "weighted_adj_tc_mean = np.where(x_mean_tc < threshold, 0, x_mean_tc)\n",
    "\n",
    "res_adhd= np.count_nonzero(weighted_adj_adhd_mean)-93\n",
    "res_tc= np.count_nonzero(weighted_adj_tc_mean)-93\n",
    "print(res_adhd, res_tc)\n",
    "\n",
    "# mean adjacency matrix of adhd and tc\n",
    "adj_adhd_mean = np.where(x_adhd[0] < threshold, 0, 1)\n",
    "adj_tc_mean = np.where(x_tc[0] < threshold, 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c05182",
   "metadata": {},
   "source": [
    "# Finding adjacency matrix based on average thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f251c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values above threshold is considered as 1, below is 0 \n",
    "X = np.where(abs(X) < threshold, 0, 1) \n",
    "np.save(\"E:/ADHD200/plv_corr/all_plv_adjacency.npy\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e596085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the binary adjacency matrix\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(adj_adhd_mean, cmap='binary', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xlabel('ROI index',fontsize=15)\n",
    "plt.ylabel('ROI index',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde44a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the binary adjacency matrix\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(adj_tc_mean, cmap='binary', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xlabel('ROI index',fontsize=15)\n",
    "plt.ylabel('ROI index',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180ee01",
   "metadata": {},
   "source": [
    "# Correlation based functional connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de9bd22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series1=np.load(\"E:/ADHD200/ICA_times_series_905.npy\",allow_pickle=True)\n",
    "label= np.load(\"E:/ADHD200/label_941.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "665df284",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series2=np.load(\"E:/ADHD200/dict_learning_times_series_905.npy\",allow_pickle=True)\n",
    "label= np.load(\"E:/ADHD200/label_941.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cbec2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = []\n",
    "for i in range(len(time_series1)):\n",
    "   time_series.append(np.concatenate((time_series1[i], time_series2[i]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "773f95c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "each_roi=[]\n",
    "all_roi=[]\n",
    "all_sub=[]\n",
    "for i in range(0,len(time_series)):\n",
    "        for j in range(0,time_series[i].shape[1]):\n",
    "            for k in range(0, time_series[i].shape[1]):\n",
    "                    corr = np.corrcoef(time_series[i][:,j], time_series[i][:,k])\n",
    "                    r = corr[0, 1]\n",
    "                    each_roi.append(r)\n",
    "            all_roi.append(each_roi)       \n",
    "            each_roi=[]\n",
    "        all_sub.append(all_roi)\n",
    "        all_roi=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9e9ddc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"E:/ADHD200/plv_corr/all_corr.npy\", all_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf421868",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.load(\"E:/ADHD200/plv_corr/all_corr.npy\")\n",
    "Y=np.load(\"E:/ADHD200/label_905.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aee1f5",
   "metadata": {},
   "source": [
    "# Calculate threshold and correlation based adjacency matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "612e937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.mean(X, axis=0)\n",
    "sum_of_values = np.sum(X_mean)\n",
    "#calculate the threshold value\n",
    "threshold= sum_of_values/(X_mean.shape[0]*X_mean.shape[1])\n",
    "X = np.where(X < threshold, 0, 1)\n",
    "X = np.where(abs(X) < threshold, 0, 1) \n",
    "np.save(\"E:/ADHD200/plv_corr/all_corr_adjacency.npy\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c18252",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adhd=[]\n",
    "x_tc=[]\n",
    "for i in range(0,len(X)):\n",
    "    if(Y[i]==1):\n",
    "        x_adhd.append(X[i])\n",
    "    else:\n",
    "        x_tc.append(X[i])\n",
    "\n",
    "x_mean_adhd = np.mean(x_adhd, axis=0)\n",
    "x_mean_tc = np.mean(x_tc, axis=0)\n",
    "\n",
    "weighted_adj_adhd_mean = np.where(x_mean_adhd < threshold, 0, x_mean_adhd)\n",
    "weighted_adj_tc_mean = np.where(x_mean_tc < threshold, 0, x_mean_tc)\n",
    "\n",
    "res_adhd= np.count_nonzero(weighted_adj_adhd_mean)-93\n",
    "res_tc= np.count_nonzero(weighted_adj_tc_mean)-93\n",
    "print(res_adhd, res_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a9b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adhd=[]\n",
    "x_tc=[]\n",
    "for i in range(0,len(X)):\n",
    "    if(Y[i]==1):\n",
    "        x_adhd.append(X[i])\n",
    "    else:\n",
    "        x_tc.append(X[i])\n",
    "\n",
    "x_mean_adhd = np.mean(x_adhd, axis=0)\n",
    "x_mean_tc = np.mean(x_tc, axis=0)\n",
    "\n",
    "# mean adjacency matrix of adhd and tc\n",
    "adj_adhd_mean = np.where(x_mean_adhd < threshold, 0, 1)\n",
    "adj_tc_mean = np.where(x_mean_tc < threshold, 0, 1)\n",
    "\n",
    "res_adhd= np.count_nonzero(adj_adhd_mean)-93\n",
    "res_tc= np.count_nonzero(adj_tc_mean)-93\n",
    "print(res_adhd, res_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588897c3",
   "metadata": {},
   "source": [
    "# Spread of correlation and plv based connectivity strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1372bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1= np.load(\"E:/ADHD200/plv_corr/all_plv.npy\")\n",
    "X2= np.load(\"E:/ADHD200/plv_corr/all_corr.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad131b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample PLV and correlation-based connectivity matrices (replace with actual data)\n",
    "X_plv =  np.mean(X1, axis=0)\n",
    "X_corr = np.mean(X2, axis=0)\n",
    "\n",
    "# Flatten the matrices to get connectivity strength values\n",
    "plv_strength = X_plv.flatten()\n",
    "corr_strength = X_corr.flatten()\n",
    "\n",
    "# Plot histograms of connectivity strength for both PLV and correlation-based matrices\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(plv_strength, bins=50, alpha=0.5, color='blue', label='PLV-Based')\n",
    "plt.hist(corr_strength, bins=50, alpha=0.5, color='green', label='Correlation-Based')\n",
    "plt.xlabel('Connectivity Strength')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Connectivity Strength')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871275a9",
   "metadata": {},
   "source": [
    "# Kullback-Leibler Divergence calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75517105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the plv connectivity matrix\n",
    "X= np.load(\"E:/ADHD200/plv_corr/all_plv.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e66254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean connectivity matrix\n",
    "X_mean = np.mean(X, axis=0)\n",
    "sum_of_values = np.sum(X_mean)\n",
    "#calculate the threshold value\n",
    "threshold= sum_of_values/(X_mean.shape[0]*X_mean.shape[1])\n",
    "x_adhd=[]\n",
    "x_tc=[]\n",
    "for i in range(0,len(X)):\n",
    "    if(Y[i]==1):\n",
    "        x_adhd.append(X[i])\n",
    "    else:\n",
    "        x_tc.append(X[i])\n",
    "\n",
    "x_mean_adhd = np.mean(x_adhd, axis=0)\n",
    "x_mean_tc = np.mean(x_tc, axis=0)\n",
    "\n",
    "\n",
    "# weighted mean matrix generation\n",
    "weighted_adj_adhd_mean = np.where(x_mean_adhd < threshold, 0, x_mean_adhd)\n",
    "weighted_adj_tc_mean = np.where(x_mean_tc < threshold, 0, x_mean_tc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "def kl_divergence(adj_matrix1, adj_matrix2, epsilon=1e-10):\n",
    "    # Normalize adjacency matrices to obtain probability distributions\n",
    "    prob_dist1 = (adj_matrix1 + epsilon) / np.sum(adj_matrix1 + epsilon, axis=1, keepdims=True)\n",
    "    prob_dist2 = (adj_matrix2 + epsilon) / np.sum(adj_matrix2 + epsilon, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute KL divergence\n",
    "    kl_div = np.sum(prob_dist1 * np.log(prob_dist1 / prob_dist2), axis=1)\n",
    "    \n",
    "    return kl_div\n",
    "\n",
    "# Example usage:\n",
    "# Assuming adj_matrix1 and adj_matrix2 are the two adjacency matrices to be compared\n",
    "\n",
    "kl_div = kl_divergence(weighted_adj_adhd_mean, weighted_adj_tc_mean)\n",
    "print(\"KL Divergence:\", np.mean(kl_div))\n",
    "print(len(kl_div))\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Assuming kl_div is your list/array of KL divergences\n",
    "\n",
    "# Plot KL divergence\n",
    "plt.scatter(range(len(kl_div)), kl_div, label='KL Divergence')\n",
    "\n",
    "# Calculate and plot average KL divergence\n",
    "average_kl_div = np.mean(kl_div)\n",
    "plt.axhline(y=average_kl_div, color='r', linestyle='--', label='Average KL Divergence')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Nodes')\n",
    "plt.ylabel('KL Divergence')\n",
    "plt.title('Kullback-Leibler Divergence')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa9997",
   "metadata": {},
   "source": [
    "# HURST Exponent calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a94019a",
   "metadata": {},
   "source": [
    "# ADHD hurst exponent calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e955a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def hurst_exponent_roi(time_series_roi):\n",
    "    \"\"\"Calculate the Hurst exponent of a time series for a single ROI.\"\"\"\n",
    "    N = len(time_series_roi)\n",
    "    \n",
    "    # Calculate the range of the time series\n",
    "    range_vals = np.ptp(time_series_roi)\n",
    "    \n",
    "    # Calculate the mean of the time series\n",
    "    mean = np.mean(time_series_roi)\n",
    "    \n",
    "    # Calculate the cumulative deviation from the mean\n",
    "    cum_dev = np.cumsum(time_series_roi - mean)\n",
    "    \n",
    "    # Calculate the range of cumulative deviation\n",
    "    R = np.max(cum_dev) - np.min(cum_dev)\n",
    "    \n",
    "    # Calculate the standard deviation of the time series\n",
    "    S = np.std(time_series_roi)\n",
    "    \n",
    "    # Calculate the rescaled range\n",
    "    R_S = R / S\n",
    "    \n",
    "    # Calculate the Hurst exponent\n",
    "    hurst = np.log(R_S) / np.log(N)\n",
    "    \n",
    "    return hurst\n",
    "\n",
    "# Example time series data with shape (a x b)\n",
    "time_series_data = np.array(tc_timeseries)  # Example data with 1000 time points and 10 ROIs\n",
    "\n",
    "# Calculate the Hurst exponent for each ROI\n",
    "hurst_exponents = []\n",
    "for roi_index in range(time_series_data[4].shape[1]):\n",
    "    hurst_roi = hurst_exponent_roi(time_series_data[4][:, roi_index])\n",
    "    hurst_exponents.append(hurst_roi)\n",
    "\n",
    "plt.rcParams.update({'font.size': 14, 'legend.fontsize': 12})\n",
    "plt.figure(figsize=(8, 6))\n",
    "x_values = range(len(hurst_exponents))\n",
    "y_values = hurst_exponents\n",
    "\n",
    "# Plot markers for nonlinear Hurst exponents\n",
    "nonlinear_indices = [i for i, hurst in enumerate(hurst_exponents) if np.abs(hurst - 0.5) > 0.1]\n",
    "plt.scatter(np.array(x_values)[nonlinear_indices], np.array(hurst_exponents)[nonlinear_indices], marker='*', color='blue', s=80, label='Nonlinear (Hurst Exponent ≠ 0.5)')\n",
    "\n",
    "# Plot markers for linear Hurst exponents\n",
    "linear_indices = [i for i, hurst in enumerate(hurst_exponents) if np.abs(hurst - 0.5) <= 0.1]\n",
    "plt.scatter(np.array(x_values)[linear_indices], np.array(hurst_exponents)[linear_indices], marker='o', color='blue', s=80, label='Linear (Hurst Exponent ~ 0.5)')\n",
    "\n",
    "# Draw a horizontal line at y=0.5\n",
    "plt.axhline(y=0.5, color='red', linestyle='--')\n",
    "\n",
    "plt.xlabel('ROI Index')\n",
    "plt.ylabel('Hurst Exponent')\n",
    "plt.title('Hurst Exponents for Each ROI')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e71bc",
   "metadata": {},
   "source": [
    "# TC hurst exponent calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1920fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def hurst_exponent_roi(time_series_roi):\n",
    "    \"\"\"Calculate the Hurst exponent of a time series for a single ROI.\"\"\"\n",
    "    N = len(time_series_roi)\n",
    "    \n",
    "    # Calculate the range of the time series\n",
    "    range_vals = np.ptp(time_series_roi)\n",
    "    \n",
    "    # Calculate the mean of the time series\n",
    "    mean = np.mean(time_series_roi)\n",
    "    \n",
    "    # Calculate the cumulative deviation from the mean\n",
    "    cum_dev = np.cumsum(time_series_roi - mean)\n",
    "    \n",
    "    # Calculate the range of cumulative deviation\n",
    "    R = np.max(cum_dev) - np.min(cum_dev)\n",
    "    \n",
    "    # Calculate the standard deviation of the time series\n",
    "    S = np.std(time_series_roi)\n",
    "    \n",
    "    # Calculate the rescaled range\n",
    "    R_S = R / S\n",
    "    \n",
    "    # Calculate the Hurst exponent\n",
    "    hurst = np.log(R_S) / np.log(N)\n",
    "    \n",
    "    return hurst\n",
    "\n",
    "# Example time series data with shape (a x b)\n",
    "time_series_data = np.array(adhd_timeseries)  # Example data with 1000 time points and 10 ROIs\n",
    "\n",
    "# Calculate the Hurst exponent for each ROI\n",
    "hurst_exponents = []\n",
    "for roi_index in range(time_series_data[10].shape[1]):\n",
    "    hurst_roi = hurst_exponent_roi(time_series_data[10][:, roi_index])\n",
    "    hurst_exponents.append(hurst_roi)\n",
    "\n",
    "plt.rcParams.update({'font.size': 18, 'legend.fontsize': 15})\n",
    "plt.figure(figsize=(8, 6))\n",
    "x_values = range(len(hurst_exponents))\n",
    "y_values = hurst_exponents\n",
    "\n",
    "# Plot markers for nonlinear Hurst exponents\n",
    "nonlinear_indices = [i for i, hurst in enumerate(hurst_exponents) if np.abs(hurst - 0.5) > 0.1]\n",
    "plt.scatter(np.array(x_values)[nonlinear_indices], np.array(hurst_exponents)[nonlinear_indices], marker='*', color='blue', s=80, label='Nonlinear (Hurst Exponent ≠ 0.5)')\n",
    "\n",
    "# Plot markers for linear Hurst exponents\n",
    "linear_indices = [i for i, hurst in enumerate(hurst_exponents) if np.abs(hurst - 0.5) <= 0.1]\n",
    "plt.scatter(np.array(x_values)[linear_indices], np.array(hurst_exponents)[linear_indices], marker='o', color='blue', s=80, label='Linear (Hurst Exponent ~ 0.5)')\n",
    "\n",
    "# Draw a horizontal line at y=0.5\n",
    "plt.axhline(y=0.599, color='red', linestyle='--')\n",
    "\n",
    "plt.xlabel('ROI Index')\n",
    "plt.ylabel('Hurst Exponent')\n",
    "plt.title('Hurst Exponents for Each ROI')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8052d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4934bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
